<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "something",
  "description": "some other thing",
  "password": "svgs",
  "authors": [
    {
      "author": "Reiichiro Nakano",
      "authorURL": "https://reiinakano.github.io/",
      "affiliation": "",
      "affiliationURL": ""
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Neural Painters</h1>
  <p>Title is a work in progress</p>
  <div class="l-page" id="vtoc"></div>
</d-title>

<d-article>

  <p>
    This post explores neural painters' strengths and weaknesses, training methods, and some interesting things that can be achieved with them.
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      <!-- <span class="figure-number">Figure 1:</span> -->
      <span style="hyphens: manual;">As long as an </span>
      <span style="background-color: #FFF1E7; padding-left: 2px; padding-right: 2px;">image para&shy;meter&shy;ization</span>
      <span>is differ&shy;entiable, we can back&shy;propagate</span>
      <span style="white-space: nowrap;">( <img src="diagrams/backprop-arrow.svg" style="width: unset;"/> )</span>
      <span>through it.</span>
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <p>
    Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
    <d-footnote>Test! Let's cite<d-cite key="gatys2015"></d-cite> someone!</d-footnote>
    More text!
  </p> 

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-training-painter-networks" class="section-number">1</a>
  </p>
  <h2 id="section-training-painter-networks"><a href="#section-training-painter-networks">Training Painter Networks</a></h2>

  <p>
    The main role of a neural painter is to serve as a fully differentiable simulation of a particular painting program.
    In this article, we use an open source painting program called <a href="http://mypaint.org">MyPaint</a>, which was also used by SPIRAL.
  </p>

  <p>
    There are two main considerations for training a neural painter, the painting program's <b>action space</b> and the neural painter's <b>architecture</b>.
  </p>

  <p>
    The <b>action space</b> defines the set of parameters that are used as control inputs for the painting environment.
    It serves as the interface that an agent can use to generate a painting.
  </p>

  <p>
    For the experiments in this article, we use a slight variation of the action space used by SPIRAL, that maps a single action to a single brushstroke in the MyPaint program.
    An agent "paints" by successively generating actions and applying full brushstrokes on a canvas.
    The table below shows the variables that constitute the action space.
  </p>

  <table>
    <tr>
      <th>Action variable</th>
      <th>Description</th>
    </tr>
    <tr>
      <td>Start and end pressure</td>
      <td>Two variables that define the pressure applied to the brush at the beginning and end of the stroke.</td> 
    </tr>
    <tr>
      <td>Brush size</td>
      <td>Determines the radius of the generated brushstroke.</td>
    </tr>
    <tr>
      <td>Color</td>
      <td>3D integer vector determining the RGB color of the brushstroke.</td>
    </tr>
    <tr>
      <td>Brush coordinates</td>
      <td>
        Three Cartesian coordinates on a 2D canvas, defining the brushstroke's shape.
        The coordinates define a starting point, end point, and an intermediate control point, constituting a quadratic Bezier curve.
      </td>
    </tr>
  </table>

  <p>
    The main difference of this action space from the one used by SPIRAL is that we avoid the use of discrete variables.
    Instead of using discrete variables for brush size and stroke pressure, we use continuous variables.
    We also drop the binary variable that determines whether or not to lift the brush.
    The reasoning behind this will be explained in more detail in a later section.
  </p>

  <p>
    The diagram below lets you explore the effect of each variable in the action space on a brushstroke rendered by MyPaint.
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      The user is allowed to move the various knobs and see how the action space defines a full brushstroke. 
      Unless there's a good way to run a C++ library on the browser (MyPaint), then the images will presumably be fetched from somewhere as the knobs move.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <p>
    The first approach for training a neural painter uses two stages: train a VAE on brush strokes to learn a latent space for brush strokes, then train a feedforward neural network to map an action to the latent space.
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      Simple animated GIF or MP4 of a real brushstroke (left) and the neurally generated brushstroke (right) moving through the action space.
      We can include the moving knobs in the animation like in Figure 2 of DiffParams article.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <p>
    The second approach trains a neural network directly from action space to brush stroke, using an adversarial loss. 
    This results in more "realistic" brushstrokes as opposed to the smoothened out VAE brushstrokes.
  </p>

  <span id="figure-number-gan-painter-exploration" class="figcaption kicker-text-align add-colab-link--section-recreating-spiral" style="grid-column: kicker; margin-top: 20px;">
    <p><a href="#figure-gan-painter-exploration" class="figure-number">3</a></p>
  </span>
  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      Simple animated GIF or MP4 of a real brushstroke (left) and the neurally generated brushstroke (right) moving through the action space.
      We can include the moving knobs in the animation like in Figure 2 of DiffParams article.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-recreating-spiral" class="section-number">2</a>
  </p>
  <h2 id='section-recreating-spiral'><a href="#section-recreating-spiral">Recreating SPIRAL results</a></h2>

  <p>
    The SPIRAL paper trained an agent to learn to recreate images from a particular domain (MNIST, Omniglot, CelebA) using a constrained action space (the painting program). 
    As the painting program is non-differentiable, the agent is trained using adversarial reinforcement learning.
  </p>

  <p> 
    Since a neural painter is fully differentiable, we don't need reinforcement learning, and can just train the agent using regular adversarial methods. 
    We test the approach's performance on three datasets: MNIST, KMNIST, and CelebA, and show the results.
  </p>

  <p>
    We begin with MNIST.
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      The diagram shows three images side by side: one of the target image, one of the neural painter output produced by the agent, and finally, the same actions executed on the real painting program. 
      The latter two images will be animated, showing the stroke order used to recreate the target image. 
      There is a button that can be clicked to fetch a new set of images.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <p>
    Then KMNIST.
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      Same as MNIST diagram.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <p>
    Finally, we do CelebA.
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      The diagram for CelebA should be almost the same as the diagrams for the previous two datasets. 
      In addition to the button for fetching a new set of images, we add a button for toggling the output produced by the VAE vs GAN neural painter, and a slider for selecting the number of strokes.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <p>
    We end with an interesting interactive diagram.
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      An interesting interactive diagram would be letting a user generate their own target image. 
      For MNIST/KMNIST, the user could draw strokes as in the handwriting article. 
      Alternatively, the user could upload an image or take a picture and the CelebA agent could attempt to draw it. 
      This will only work if the agent and neural painter are small enough to be run on TF.js.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <h3>
    The effect of discrete actions
  </h3>

  <p>
    This is a short aside on discrete action spaces. 
    So far, we've constrained the brushstroke action space to continuous variables. 
    This is in contrast to the original SPIRAL paper, that used 10 discrete levels for representing brush pressure and a binary flag that determines whether the brush is lifted or not.
  </p>

  <p>
    In this section, a neural painter trained with discrete actions is explored. 
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      This diagram shows the effect of lifting the brush "partially" (value between 0 and 1). 
      An agent using this neural painter fails immediately (no strokes produced) when trained using the same adversarial method since the gradient caused by the jump action is not meaningful.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-expert-teaching" class="section-number">3</a>
  </p>
  <h2 id='section-expert-teaching'><a href="#section-expert-teaching">Learning Human Strokes</a></h2>

  <p>
    Previously, the agent can be observed to favor a particular stroke order e.g. top to bottom, left to right for MNIST digits. 
    This section shows that if the agent is preconditioned on a specific stroke order (requires only one example per class), it learns to apply that stroke order when recreating other examples.
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      This is just an animated GIF of the strokes used to pre-condition the agent. 
      Notice there is only one sample per class.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      This diagram is similar to the previous diagrams on recreating SPIRAL experiments. 
      In addition to the "Load New Image" button, the user can select a conditional category used as input to the agent. 
      This will affect the path the agent takes when attempting to recreate the digit.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-diff-image-param" class="section-number">4</a>
  </p>
  <h2 id='section-diff-image-param'><a href="#section-diff-image-param">As a Differentiable Image Parameterization</a></h2>

  <p>
    Since the neural painter is differentiable, it can be used as a differentiable image parameterization. 
    The diagrams here will mostly mirror the ones from the Differentiable Image Parameterization article.
  </p>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      This is a zoomable collection of pictures, similar to the CPPN example results in Figure 7 of the DiffParams article. 
      One interesting addition could be a dropdown of categories. 
      Upon selection of a category, all images change to an optimized "painting" of that category.
      Zooming into one of the images could also show how it was generated e.g. number of strokes, VAE/GAN painter.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>

  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      We can show some videos of the optimization from a random initialization to end result.
    </figcaption>
    <div class="l-body">
      <%= require("../static/diagrams/general.svg") %>
    </div>
  </figure>
  
  <hr/>

  <h2 id='conclusions'><a href="#conclusions">Conclusions</a></h2>

  <p>
    Constraints are a key element of creativity. 
    The natural constraints that an artistic medium imposes on the artist give a piece of art a distinct look from others. 
    An artist attempting to paint a scene using oil paints will get a remarkably distinct result from someone trying to sketch the same scene using a single pencil. 
    In the same sense, an agent constrained to use brush strokes will find creative ways to achieve its objective (recreate a digit, face, or maximize another neural network's activations). 
    This article explored the power of neural painters as a differentiable constraint learned from a real painting simulator, but the techniques and ideas introduced can be applied to other mediums, perhaps 3D renderers or animations.
  </p>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to...
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
